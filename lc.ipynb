{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d329f5e4",
   "metadata": {},
   "source": [
    "# LangChain Foundations ‚Äî LLM ‚Üí Memory ‚Üí RAG ‚Üí Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5d293",
   "metadata": {},
   "source": [
    "## Setup & VS Code notes\n",
    "1. Create venv (if not already):\n",
    "```bash\n",
    "python -m venv ollama-env\n",
    "```\n",
    "2. Activate venv:\n",
    "- Windows: `ollama-env\\Scripts\\activate`\n",
    "- Mac/Linux: `source ollama-env/bin/activate`\n",
    "\n",
    "3. In VS Code: `Ctrl+Shift+P` ‚Üí `Python: Select Interpreter` ‚Üí pick `ollama-env`.\n",
    "4. Install packages (run the cell below). Jupyter inside VS Code will use the selected interpreter/venv.\n",
    "\n",
    "> **Note:** This notebook targets Python 3.10+. The code avoids syntax requiring newer versions. If running on a different 3.x, it should still work."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f50ad67b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys, subprocess, pkgutil\n",
    "\n",
    "required = [\n",
    "    'langchain>=0.0.200', 'langchain-core', 'langchain-community',\n",
    "    'chromadb', 'faiss-cpu', 'sentence-transformers', 'openai',\n",
    "    'ollama', 'pypdf', 'tqdm'\n",
    "]\n",
    "\n",
    "def install(pkg):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "for pkg in required:\n",
    "    name = pkg.split('>=')[0]\n",
    "    if pkgutil.find_loader(name) is None:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        install(pkg)\n",
    "    else:\n",
    "        print(f\"{name} already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94f5fe",
   "metadata": {},
   "source": [
    "## LangChain ‚Äî LLM Wrapper (Why we use it)\n",
    "- LangChain provides a consistent interface for many LLMs (OpenAI, Ollama, Bedrock, HF, local models).\n",
    "- You get prompt templates, chains, memory helpers, tools, and agents with consistent APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b9daf",
   "metadata": {},
   "source": [
    "#Connecting LangChain with Ollama\n",
    "\n",
    "LangChain connects to different Large Language Models (LLMs) through wrappers.\n",
    "These wrappers make it easy to send a prompt and receive a model response\n",
    "without handling HTTP requests manually.\n",
    "\n",
    "For local models, **Ollama** is the easiest option.  \n",
    "It allows running open-source models like *Mistral, Llama 3, Phi-3, Gemma,* etc. directly on your system.\n",
    "\n",
    "LangChain‚Äôs **`Ollama`** class (from `langchain_community.llms`) provides a simple API\n",
    "to send prompts to any Ollama model.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Basic Flow\n",
    "1. Ollama service runs locally at `http://localhost:11434`\n",
    "2. LangChain‚Äôs `Ollama` wrapper connects to it\n",
    "3. You call `.invoke(prompt)` or `.generate([...])` to get model output\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Key Advantages\n",
    "- No API keys or cloud costs\n",
    "- Works offline\n",
    "- Fast for prototyping and teaching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3464fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_6336\\3400723654.py:6: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"mistral\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå¨Ô∏è Greetings! I am Mistral, the gentle wind of wisdom. Let's sail through knowledge together! ‚öìÔ∏èüåä\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Test LangChain <-> Ollama connection\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize the local LLM connection\n",
    "# This will connect to the Ollama daemon running on localhost\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Send a simple prompt\n",
    "response = llm.invoke(\"Say a one-line hello message from Mistral!\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ab309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Setting optional parameters\n",
    "llm = Ollama(model=\"mistral\", temperature=0.7)\n",
    "\n",
    "prompt = \"List three advantages of using containerization in IT infrastructure.\"\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f61d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[GenerationChunk(text=' Virtualization is the creation of a software-defined version of something, typically a computer system resource or an operating system, that acts like a real instance but exists as part of a larger, abstracted system.', generation_info={'model': 'mistral', 'created_at': '2025-11-10T06:04:30.8566509Z', 'response': '', 'done': True, 'done_reason': 'stop', 'context': [3, 29473, 14470, 1194, 9020, 2605, 1065, 1392, 2175, 29491, 4, 1027, 19800, 2605, 1117, 1040, 10081, 1070, 1032, 4698, 29501, 12266, 3519, 1070, 2313, 29493, 10334, 1032, 6842, 2355, 4483, 1210, 1164, 11281, 2355, 29493, 1137, 12668, 1505, 1032, 2121, 4103, 1330, 7376, 1158, 1512, 1070, 1032, 6852, 29493, 12344, 1054, 2355, 29491], 'total_duration': 25855660900, 'load_duration': 15377012400, 'prompt_eval_count': 13, 'prompt_eval_duration': 1772317100, 'eval_count': 43, 'eval_duration': 8704352500})], [GenerationChunk(text=' DevOps is a software development practice that emphasizes communication, collaboration, and integration between software developers (Dev) and IT professionals (Ops) to streamline the software delivery process. The goal of DevOps is to create a culture and environment where building, testing, and deploying high-quality software can happen more rapidly, frequently, and reliably, resulting in faster innovation, improved quality, reduced time to market, and better alignment with business objectives. This approach often involves the use of tools and practices such as continuous integration, continuous delivery, infrastructure as code, containerization, and automation to improve efficiency and reduce errors. DevOps also encourages a focus on measurement and feedback, using data-driven insights to continuously improve processes and outcomes over time.', generation_info={'model': 'mistral', 'created_at': '2025-11-10T06:05:03.7337232Z', 'response': '', 'done': True, 'done_reason': 'stop', 'context': [3, 29473, 2592, 1117, 7392, 24940, 29572, 4, 1027, 7392, 24940, 1117, 1032, 4698, 4867, 6013, 1137, 11342, 5772, 9288, 29493, 16377, 29493, 1072, 15446, 2212, 4698, 18660, 1093, 5759, 29499, 1072, 9630, 13517, 1093, 24940, 29499, 1066, 5222, 1849, 1040, 4698, 10330, 2527, 29491, 1183, 6309, 1070, 7392, 24940, 1117, 1066, 2999, 1032, 6447, 1072, 5234, 1738, 4435, 29493, 9028, 29493, 1072, 16026, 1056, 2254, 29501, 15585, 4698, 1309, 5572, 1448, 14867, 29493, 11988, 29493, 1072, 23657, 2675, 29493, 11271, 1065, 10324, 17631, 29493, 12725, 4813, 29493, 10165, 1495, 1066, 3436, 29493, 1072, 2641, 20260, 1163, 2723, 22902, 29491, 1619, 5199, 3376, 15425, 1040, 1706, 1070, 7808, 1072, 11647, 2027, 1158, 13502, 15446, 29493, 13502, 10330, 29493, 15229, 1158, 3464, 29493, 8408, 2605, 29493, 1072, 5375, 1120, 1066, 5684, 13600, 1072, 8411, 8327, 29491, 7392, 24940, 1603, 6959, 2059, 1032, 4000, 1124, 16689, 1072, 12907, 29493, 2181, 1946, 29501, 28137, 21483, 1066, 26551, 5684, 10305, 1072, 18782, 1522, 1495, 29491], 'total_duration': 30852889800, 'load_duration': 192837600, 'prompt_eval_count': 10, 'prompt_eval_duration': 998613700, 'eval_count': 153, 'eval_duration': 29660281700})], [GenerationChunk(text=' APIs, or Application Programming Interfaces, are a set of rules and protocols that allow different software applications to talk to each other. They define the methods and data formats that one software application can use to interact with another, enabling them to share and exchange information effectively without having to know the internal details of how the other functions. In simple terms, APIs act as a messenger or translator between two different software systems. They help streamline integration, making it easier for multiple software applications to work together seamlessly.', generation_info={'model': 'mistral', 'created_at': '2025-11-10T06:05:28.5582884Z', 'response': '', 'done': True, 'done_reason': 'stop', 'context': [3, 29473, 2592, 1228, 11270, 3069, 1065, 4356, 4239, 29572, 4, 1027, 11270, 3069, 29493, 1210, 13250, 8115, 4850, 5055, 10056, 29493, 1228, 1032, 1576, 1070, 6647, 1072, 10957, 29481, 1137, 2682, 2349, 4698, 9197, 1066, 2753, 1066, 2198, 1567, 29491, 2074, 7368, 1040, 6330, 1072, 1946, 24236, 1137, 1392, 4698, 5761, 1309, 1706, 1066, 14881, 1163, 2466, 29493, 26516, 1474, 1066, 4866, 1072, 9645, 2639, 12234, 2439, 3229, 1066, 1641, 1040, 6525, 4930, 1070, 1678, 1040, 1567, 6340, 29491, 1328, 4356, 4239, 29493, 11270, 3069, 1728, 1158, 1032, 5455, 10011, 1210, 8022, 1796, 2212, 1757, 2349, 4698, 5686, 29491, 2074, 2084, 5222, 1849, 15446, 29493, 3260, 1146, 7857, 1122, 5934, 4698, 9197, 1066, 1539, 3321, 24646, 13066, 29491], 'total_duration': 22781868100, 'load_duration': 17530100, 'prompt_eval_count': 13, 'prompt_eval_duration': 1301447700, 'eval_count': 108, 'eval_duration': 21462330500})]]\n",
      "Prompt 1: Explain virtualization in one line.\n",
      "Response:  Virtualization is the creation of a software-defined version of something, typically a computer system resource or an operating system, that acts like a real instance but exists as part of a larger, abstracted system. \n",
      "\n",
      "Prompt 2: What is DevOps?\n",
      "Response:  DevOps is a software development practice that emphasizes communication, collaboration, and integration between software developers (Dev) and IT professionals (Ops) to streamline the software delivery process. The goal of DevOps is to create a culture and environment where building, testing, and deploying high-quality software can happen more rapidly, frequently, and reliably, resulting in faster innovation, improved quality, reduced time to market, and better alignment with business objectives. This approach often involves the use of tools and practices such as continuous integration, continuous delivery, infrastructure as code, containerization, and automation to improve efficiency and reduce errors. DevOps also encourages a focus on measurement and feedback, using data-driven insights to continuously improve processes and outcomes over time. \n",
      "\n",
      "Prompt 3: What are APIs in simple terms?\n",
      "Response:  APIs, or Application Programming Interfaces, are a set of rules and protocols that allow different software applications to talk to each other. They define the methods and data formats that one software application can use to interact with another, enabling them to share and exchange information effectively without having to know the internal details of how the other functions. In simple terms, APIs act as a messenger or translator between two different software systems. They help streamline integration, making it easier for multiple software applications to work together seamlessly. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.generations)\n",
    "for i, gen in enumerate(result.generations):\n",
    "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "    print(\"Response:\", gen[0].text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69b6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Explain virtualization in one line.\n",
      "Response:  Virtualization is the creation of a software-defined version of something, typically a computer system resource or an operating system, that acts like a real instance but exists as part of a larger, abstracted system. \n",
      "\n",
      "Prompt 2: What is DevOps?\n",
      "Response:  DevOps is a software development practice that emphasizes communication, collaboration, and integration between software developers (Dev) and IT professionals (Ops) to streamline the software delivery process. The goal of DevOps is to create a culture and environment where building, testing, and deploying high-quality software can happen more rapidly, frequently, and reliably, resulting in faster innovation, improved quality, reduced time to market, and better alignment with business objectives. This approach often involves the use of tools and practices such as continuous integration, continuous delivery, infrastructure as code, containerization, and automation to improve efficiency and reduce errors. DevOps also encourages a focus on measurement and feedback, using data-driven insights to continuously improve processes and outcomes over time. \n",
      "\n",
      "Prompt 3: What are APIs in simple terms?\n",
      "Response:  APIs, or Application Programming Interfaces, are a set of rules and protocols that allow different software applications to talk to each other. They define the methods and data formats that one software application can use to interact with another, enabling them to share and exchange information effectively without having to know the internal details of how the other functions. In simple terms, APIs act as a messenger or translator between two different software systems. They help streamline integration, making it easier for multiple software applications to work together seamlessly. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using .generate() for multiple prompts\n",
    "prompts = [\n",
    "    \"Explain virtualization in one line.\",\n",
    "    \"What is DevOps?\",\n",
    "    \"What are APIs in simple terms?\"\n",
    "]\n",
    "\n",
    "result = llm.generate(prompts)\n",
    "\n",
    "# .generate() returns an object with generations for each prompt\n",
    "for i, gen in enumerate(result.generations):\n",
    "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "    print(\"Response:\", gen[0].text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Streaming output (useful for chat UIs)\n",
    "for chunk in llm.stream(\"Explain microservices architecture in short.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "381d2882",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Prompt Templates in LangChain\n",
    "LangChain uses *Prompt Templates* to create structured prompts with variables, \n",
    "making your prompts reusable and dynamic. \n",
    "Templates help you inject user inputs or system variables into consistent prompt patterns.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e35a02d",
   "metadata": {},
   "source": [
    "# üß© Topic 2: Prompt Templates in LangChain\n",
    "\n",
    "LangChain uses **Prompt Templates** to make prompts reusable, structured, and dynamic.  \n",
    "Instead of hardcoding plain text prompts, you define a template with variables that get filled \n",
    "at runtime.\n",
    "\n",
    "A **Prompt Template**:\n",
    "- Keeps prompts consistent across use-cases  \n",
    "- Allows injecting dynamic values (like user input, ticket ID, etc.)  \n",
    "- Helps separate logic from text\n",
    "\n",
    "This is essential in IT scenarios where prompts must handle user-specific or system-specific \n",
    "data (e.g., customer queries, log summaries, ticket details).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Key Classes\n",
    "| Class | Purpose |\n",
    "|--------|----------|\n",
    "| `PromptTemplate` | Template for text-based prompts |\n",
    "| `ChatPromptTemplate` | Template for chat-style inputs (for chat models) |\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Basic Syntax\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Explain {topic} in simple terms for a new IT trainee.\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "\n",
    "Then you can fill it dynamically:\n",
    "prompt.format(topic=\"virtualization\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b06432c6",
   "metadata": {},
   "source": [
    "‚úÖ Benefits\n",
    "\n",
    "Cleaner and reusable code\n",
    "\n",
    "Makes multi-turn workflows easier to maintain\n",
    "\n",
    "Enables chaining prompts dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5202a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf8c16dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PromptTemplate.format of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain {topic} in simple terms for a new IT trainee.')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1156d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      " Explain cloud computing in simple terms for a new IT trainee.\n",
      "\n",
      "Model Response:\n",
      "  Cloud computing is like a big, virtual computer that you can access from anywhere and anytime, over the internet. Instead of storing data on your personal computer or a local server, it's stored on remote servers called 'cloud servers.' These servers are managed by companies known as cloud service providers.\n",
      "\n",
      "Think about it like using an electric company to power your home instead of having your own power plant. You don't have to worry about generating electricity; you simply use it whenever you need it. The same principle applies to cloud computing ‚Äì you can use computer resources, such as storage and processing power, when you want them, without worrying about maintaining the hardware yourself.\n",
      "\n",
      "Cloud services come in three main flavors: Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS). SaaS provides software applications like email or office tools over the internet. PaaS gives developers a platform to build and deploy their own applications without worrying about infrastructure management. IaaS allows you to rent entire virtual machines and other hardware resources on demand.\n",
      "\n",
      "In summary, cloud computing is all about using shared computing resources and paying for only what you need, rather than buying, maintaining, and managing your own hardware and software. This enables businesses and individuals to be more flexible, efficient, and cost-effective with their IT needs.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Create a simple template\n",
    "template = \"Explain {topic} in simple terms for a new IT trainee.\"\n",
    "\n",
    "# Define the prompt template with the variable 'topic'\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "\n",
    "# Fill the template with a topic\n",
    "final_prompt = prompt.format(topic=\"cloud computing\")\n",
    "\n",
    "print(\"Generated Prompt:\\n\", final_prompt)\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(final_prompt)\n",
    "print(\"\\nModel Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589aba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      " \n",
      "You are an IT trainer. \n",
      "Explain the concept of containerization and give one real-world DevOps example.\n",
      "\n",
      "\n",
      "Model Response:\n",
      "  As an IT trainer, I'm happy to explain the concept of containerization! Containerization is a method of software deployment and execution that bundles an application, its dependencies (libraries, system tools, settings), and configuration files into a single, portable unit called a container. This container can run consistently across different computing environments without worrying about compatibility issues between systems, making it a popular choice for DevOps teams.\n",
      "\n",
      "   Containers allow developers to package an application with all of its necessary components and dependencies in a way that ensures the app will run reliably when moved from one environment to another. They provide isolation at the process level, so multiple containers can run on the same host while sharing resources (such as the operating system kernel) without interfering with each other.\n",
      "\n",
      "   One real-world DevOps example of containerization is using Docker and Kubernetes in a continuous integration/continuous deployment (CI/CD) pipeline. Let's take a simple scenario:\n",
      "\n",
      "   1. A developer writes code, pushes it to a source control repository like GitHub or Bitbucket.\n",
      "   2. A CI server detects the change in the repository and triggers the build process.\n",
      "   3. The CI server uses Docker to create an image of the application, which includes all necessary dependencies.\n",
      "   4. Once the image is built, it can be pushed to a container registry like Docker Hub or Amazon ECR for storage and later deployment.\n",
      "   5. On the deployment side, Kubernetes (or another orchestration tool) manages the cluster of nodes where applications are running. It deploys the images from the registry onto different machines based on predefined rules and scales them up or down as needed to maintain optimal performance.\n",
      "\n",
      "   This workflow ensures that the application can be easily deployed, scaled, and managed in a consistent manner across multiple environments (e.g., development, staging, production). Containerization simplifies the deployment process and improves collaboration between development and operations teams.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Using multiple variables\n",
    "# Template with two variables\n",
    "template = \"\"\"\n",
    "You are an IT trainer. \n",
    "Explain the concept of {technology} and give one real-world {application_area} example.\n",
    "\"\"\"\n",
    "# Define prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"technology\", \"application_area\"]\n",
    ")\n",
    "# Fill the variables dynamically\n",
    "filled_prompt = prompt.format(\n",
    "    technology=\"containerization\",\n",
    "    application_area=\"DevOps\"\n",
    ")\n",
    "print(\"Generated Prompt:\\n\", filled_prompt)\n",
    "# Send to model\n",
    "response = llm.invoke(filled_prompt)\n",
    "print(\"\\nModel Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded4f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Microservices is an architectural style that breaks down a single application into loosely coupled, independent services, each running its own process and addressing a specific business capability. This approach allows for greater scalability, flexibility, and resilience in software development. For instance, in a banking system, microservices could be used to handle different functionalities such as account management, transaction processing, loan origination, or user authentication separately, enhancing the overall efficiency and reliability of the application.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Function for reusable prompt generation\n",
    "def explain_it_concept(topic, example_domain):\n",
    "    template = \"\"\"\n",
    "    You are an IT mentor.\n",
    "    Explain the concept of {topic} in 4-5 lines with a relevant example from {example_domain}.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"topic\", \"example_domain\"])\n",
    "    final_prompt = prompt.format(topic=topic, example_domain=example_domain)\n",
    "    return llm.invoke(final_prompt)\n",
    "\n",
    "# Test the function\n",
    "print(explain_it_concept(\"microservices\", \"banking systems\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368bb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5305f0ea",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Output Parsers\n",
    "- Use OutputParsers to force structured outputs (JSON, key:value) so downstream systems can \n",
    "consume them reliably.\n",
    "- Example: enforce `{{'name':'...', 'period':'...' }}` JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9630acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use parser.parse(model_output) in exercises to validate structure.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class KingInfo(BaseModel):\n",
    "    name: str\n",
    "    reign_period: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=KingInfo)\n",
    "\n",
    "# Example usage (pseudo): instruct the model to return JSON and feed to parser\n",
    "print('Use parser.parse(model_output) in exercises to validate structure.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da2aba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# Automatically validates and converts data\n",
    "user = User(name=\"Alice\", age=\"30\")  # string \"30\" becomes int 30\n",
    "print(user.age)  # 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a119fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"Name of the king\", \"title\": \"Name\", \"type\": \"string\"}, \"country\": {\"description\": \"Country they ruled\", \"title\": \"Country\", \"type\": \"string\"}}, \"required\": [\"name\", \"country\"]}\n",
      "```\n",
      "name='King Arthur' country='apple'\n",
      "{'name': 'King Arthur', 'country': 'apple'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_6336\\1743522005.py:24: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  print(parsed.dict())\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define structure of expected output\n",
    "class KingInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the king\")\n",
    "    country: str = Field(..., description=\"Country they ruled\")\n",
    "\n",
    "# Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=KingInfo)\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "# Example model output\n",
    "model_output = \"\"\"\n",
    "{\n",
    "    \"name\": \"King Arthur\",\n",
    "    \"country\": \"apple\"\n",
    "}\n",
    "\"\"\"\n",
    "# Parse and validate\n",
    "parsed = parser.parse(model_output)\n",
    "print(parsed)\n",
    "# KingInfo(name='King Arthur', country='Britain')\n",
    "print(parsed.dict())\n",
    "# {'name': 'King Arthur', 'country': 'Britain'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str = Field(..., min_length=2)\n",
    "    age: int = Field(..., gt=0, lt=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class Country(str, Enum):\n",
    "    britain = \"Britain\"\n",
    "    france = \"France\"\n",
    "    egypt = \"Egypt\"\n",
    "\n",
    "class KingInfo(BaseModel):\n",
    "    name: str\n",
    "    country: Country"
   ]
  },
  {
   "cell_type": "raw",
   "id": "220089c8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîó Topic 3: Chains in LangChain\n",
    "\n",
    "**Chains** are one of the core building blocks in LangChain.\n",
    "\n",
    "They allow you to **connect multiple components** (like prompts, LLMs, memory, or logic) \n",
    "to build a step-by-step flow ‚Äî much like a pipeline in data processing.\n",
    "\n",
    "Each \"link\" in a chain can:\n",
    "- Take some input (e.g., a topic name)\n",
    "- Pass it to a model or template\n",
    "- Produce output for the next step\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Common Chain Types\n",
    "\n",
    "| Chain Type | Description | Use Case |\n",
    "|-------------|--------------|-----------|\n",
    "| `LLMChain` | Simplest chain ‚Äî connects a prompt to an LLM | One-step generation (e.g., summarization) |\n",
    "| `SequentialChain` | Runs multiple chains in sequence; passes outputs forward | Multi-step reasoning (e.g., summarize ‚Üí rephrase) |\n",
    "| `SimpleSequentialChain` | Similar to SequentialChain but easier ‚Äî assumes one input and one output | Quick text pipelines |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Why Chains Matter in IT Context\n",
    "\n",
    "- Standardize workflows (e.g., log summarization ‚Üí alert message)\n",
    "- Automate multi-step reasoning (e.g., incident summary ‚Üí email draft)\n",
    "- Build modular pipelines (easy to maintain or extend)\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Concept Analogy\n",
    "Think of a **chain** as a *LinkedIn recruiter flow*:\n",
    "1. Input ‚Üí Candidate data  \n",
    "2. Step 1 ‚Üí Analyze skill match  \n",
    "3. Step 2 ‚Üí Draft personalized message  \n",
    "4. Output ‚Üí Send email  \n",
    "\n",
    "Each step feeds into the next ‚Äî exactly how LangChain‚Äôs Chains work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123acdff",
   "metadata": {},
   "source": [
    "LangChain historically offered many special-purpose **chains** (SQL, API, Router, Summarize, etc.).  \n",
    "As the ecosystem matured, these have consolidated into three modern layers:\n",
    "\n",
    "- **Runnables** ‚Äî compose logic flexibly (the new foundation)\n",
    "- **RAG** ‚Äî retrieval-based tasks (production knowledge assistants)\n",
    "- **Agents** ‚Äî dynamic decision-making + tool use\n",
    "\n",
    "We will still learn the **core chains** because they teach the foundations of orchestration.  \n",
    "Everything else you‚Äôll recognize conceptually (for legacy code), but you won‚Äôt need to implement.\n",
    "\n",
    "---\n",
    "## ‚úÖ Learn & Implement (core)\n",
    "- **LLMChain** ‚Äî prompt + model (one step)\n",
    "- **SimpleSequentialChain** ‚Äî single input ‚Üí multi-step ‚Üí single output\n",
    "- **SequentialChain** ‚Äî multi-input/multi-output pipelines\n",
    "- **Summarization chains**  \n",
    "  - **Stuff** (small inputs)  \n",
    "  - **Map-Reduce** (scales across chunks)  \n",
    "  - **Refine** (iterative, detail-preserving)\n",
    "\n",
    "## ‚öôÔ∏è Know Conceptually (legacy/common, replaced by modern patterns)\n",
    "- **Retrieval/QA chains**: `RetrievalQA`, `ConversationalRetrievalChain`  \n",
    "  ‚Üí *Replaced by* **RAG** (retriever + prompt + LLM via Runnables)\n",
    "- **Routing/selection**: `RouterChain`, `MultiPromptChain`, `LLMRouterChain`  \n",
    "  ‚Üí *Replaced by* **LangGraph routers** / **Supervisor agents**\n",
    "- **Data & API**: `SQLDatabaseChain`, `APIChain`, `PAL`  \n",
    "  ‚Üí *Replaced by* **Agents + Tools** (SQL tools, API tools, code tools)\n",
    "- **Guardrails/policy**: `ConstitutionalChain`  \n",
    "  ‚Üí *Replaced by* **output parsers/validators** and hosted guardrails\n",
    "- **Glue/observability**: `TransformChain`, `AnalyzeDocumentChain`  \n",
    "  ‚Üí *Replaced by* **RunnableLambda/Map** and tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d43727",
   "metadata": {},
   "source": [
    "# üîó Topic 3: Workflows with Runnables (Modern Replacement for Chains)\n",
    "\n",
    "LangChain used to use ‚ÄúChains‚Äù like `LLMChain`, `SequentialChain`, and `MapReduceChain` to connect prompts and models.  \n",
    "In 2025+, these have been replaced by **Runnables** ‚Äî a lighter, composable abstraction that does the same job more flexibly.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è What Are Runnables?\n",
    "\n",
    "A **Runnable** is any object that:\n",
    "- Takes input data\n",
    "- Processes it (prompt formatting, logic, model call)\n",
    "- Produces output\n",
    "\n",
    "You can combine Runnables using the `|` (pipe) operator, just like UNIX pipes.  \n",
    "This gives you the same pipeline behavior as classic Chains but works with the latest LangChain versions.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Common Runnable Types\n",
    "\n",
    "| Runnable | Purpose | Example |\n",
    "|-----------|----------|---------|\n",
    "| `RunnableLambda` | Wraps a custom Python function | Transform or preprocess data |\n",
    "| `RunnablePassthrough` | Passes data through unchanged | Use as pipeline start |\n",
    "| `RunnableMap` | Run multiple branches in parallel | e.g., summary + quiz |\n",
    "| `RunnableSequence` | Combine multiple runnables sequentially | multi-step workflow |\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Modern Replacements\n",
    "\n",
    "| Classic Chain | Modern Equivalent |\n",
    "|----------------|------------------|\n",
    "| `LLMChain` | `RunnablePassthrough() | PromptTemplate | LLM` |\n",
    "| `SequentialChain` | `RunnableSequence` |\n",
    "| `SimpleSequentialChain` | Chained `|` pipes |\n",
    "| `MapReduceChain` | `RunnableMap` + summarizer |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Why This Matters\n",
    "\n",
    "- Fully compatible with **LangChain 0.4+**\n",
    "- More modular and debuggable\n",
    "- Standard for **RAG** and **Agents** internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DevOps is an approach that combines software development (Dev) and operations (Ops) practices to shorten the system's development life cycle, while simultaneously improving the quality, reliability, and robustness of software systems. By fostering collaboration and communication between these two teams, DevOps aims to speed up the delivery of applications and services, making them more efficient and responsive to customer needs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Explain the concept of {topic} in 3 lines suitable for a new IT employee.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# Runnable pipeline: Input ‚Üí Prompt ‚Üí LLM\n",
    "#chain = RunnablePassthrough() | (lambda x: prompt.format(**x)) | llm\n",
    "chain = RunnablePassthrough() | prompt | llm\n",
    "response = chain.invoke({\"topic\": \"DevOps\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# 1Ô∏è‚É£ Create model (Ollama + mistral)\n",
    "model = Ollama(model=\"mistral\")\n",
    "\n",
    "# 2Ô∏è‚É£ Create parser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# 3Ô∏è‚É£ Create prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"List three fruits {format_instructions}\"\n",
    ")\n",
    "\n",
    "# 4Ô∏è‚É£ Build Runnable chain\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 5Ô∏è‚É£ Run it\n",
    "result = chain.invoke({\"format_instructions\": parser.get_format_instructions()})\n",
    "print(result)\n",
    "# Example: ['apple', 'banana', 'cherry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a341be5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# 1Ô∏è‚É£ Define structured schema\n",
    "class KingInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the king\")\n",
    "    country: str = Field(..., description=\"Country they ruled\")\n",
    "\n",
    "# 2Ô∏è‚É£ Create parser\n",
    "parser = PydanticOutputParser(pydantic_object=KingInfo)\n",
    "\n",
    "# 3Ô∏è‚É£ Create model (Ollama mistral)\n",
    "model = Ollama(model=\"mistral\")\n",
    "\n",
    "# 4Ô∏è‚É£ Create prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Extract information about the king from this text:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "# 5Ô∏è‚É£ Build Runnable chain\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# 6Ô∏è‚É£ Invoke it\n",
    "text = \"King Arthur was a legendary British leader from Camelot.\"\n",
    "result = chain.invoke({\n",
    "    \"text\": text,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(result)\n",
    "# KingInfo(name='King Arthur', country='Britain')\n",
    "\n",
    "print(result.dict())\n",
    "# {'name': 'King Arthur', 'country': 'Britain'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef4ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='King Arthur' country=<Country.britain: 'Britain'>\n",
      "{'name': 'King Arthur', 'country': <Country.britain: 'Britain'>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3708\\979823893.py:59: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  print(result.dict())\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "# 1Ô∏è‚É£ Define strict schema with Enum for validation\n",
    "class Country(str, Enum):\n",
    "    britain = \"Britain\"\n",
    "    france = \"France\"\n",
    "    egypt = \"Egypt\"\n",
    "\n",
    "class KingInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"Name of the king\")\n",
    "    country: Country = Field(..., description=\"Country they ruled\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=KingInfo)\n",
    "\n",
    "# 2Ô∏è‚É£ Create model\n",
    "model = Ollama(model=\"mistral\")\n",
    "\n",
    "# 3Ô∏è‚É£ Create prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Extract information about the king from this text:\n",
    "{text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "# 4Ô∏è‚É£ Build the base chain\n",
    "base_chain = prompt | model | parser\n",
    "\n",
    "# 5Ô∏è‚É£ Retry wrapper\n",
    "def safe_invoke(inputs):\n",
    "    try:\n",
    "        return base_chain.invoke(inputs)\n",
    "    except OutputParserException as e:\n",
    "        # Retry by adding clarification to the prompt\n",
    "        print(\"‚ö†Ô∏è Parsing failed, retrying...\")\n",
    "        inputs[\"text\"] += (\n",
    "            \"\\n\\nPlease reformat your answer as valid JSON and choose a valid country \"\n",
    "            \"from: Britain, France, or Egypt.\"\n",
    "        )\n",
    "        return base_chain.invoke(inputs)\n",
    "\n",
    "# Wrap in a RunnableLambda for composability\n",
    "safe_chain = RunnablePassthrough() | RunnableLambda(safe_invoke)\n",
    "\n",
    "# 6Ô∏è‚É£ Run it\n",
    "text = \"King Arthur was a mythical British leader of Camelot, not France or Egypt.\"\n",
    "result = safe_chain.invoke({\n",
    "    \"text\": text,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(result)\n",
    "print(result.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd44a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'example': \" Example: Amazon's Shopping Cart System\\n\\nAmazon uses Microservices Architecture for its shopping cart system. The system is divided into several microservices such as User Service (handles user authentication and account management), Product Catalog Service (manages product information), Shopping Cart Service (manages the shopping cart, inventory tracking, pricing, etc.), Payment Service (handles payment processing), and Shipping Service (coordinates shipping and delivery).\\n\\nEach of these microservices can be developed, deployed, and scaled independently without affecting the other services. This provides several benefits such as increased scalability, resilience, maintainability, and flexibility to adapt quickly to changing business needs. For instance, if Amazon wants to introduce a new payment method, they only need to modify the Payment Service without worrying about impacting other parts of the system.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Step 1 ‚Äì Summarize\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"Summarize {topic} in one line.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "# Step 2 ‚Äì Give an IT example\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Give one real-world IT example for this summary: {summary}\",\n",
    "    input_variables=[\"summary\"]\n",
    ")\n",
    "\n",
    "# Step 3 ‚Äì Build sequential workflow\n",
    "def summarize(inputs):\n",
    "    return {\"summary\": llm.invoke(prompt1.format(**inputs))}\n",
    "\n",
    "def example(inputs):\n",
    "    return {\"example\": llm.invoke(prompt2.format(**inputs))}\n",
    "\n",
    "#workflow = RunnableSequence(first=RunnableLambda(summarize), middle=[], last=RunnableLambda(example))\n",
    "ex = RunnableSequence()\n",
    "result = workflow.invoke({\"topic\": \"microservices architecture\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b25e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': ' Virtualization is a technology that allows multiple operating systems or applications to run concurrently on a single physical computer by creating separate, simulated environments known as virtual machines (VMs). This enhances resource utilization, efficiency, and flexibility in computing.', 'question': ' \"Can you explain the differences between Type 1 and Type 2 hypervisors, giving examples of each and discussing their respective strengths and limitations?\"'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "prompt_summary = PromptTemplate.from_template(\"Summarize {topic} in 2 lines.\")\n",
    "prompt_question = PromptTemplate.from_template(\"Create one interview question on {topic}.\")\n",
    "\n",
    "parallel_chain = RunnableMap({\n",
    "    \"summary\": prompt_summary | llm,\n",
    "    \"question\": prompt_question | llm\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({\"topic\": \"virtualization\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28438044",
   "metadata": {},
   "source": [
    "# üß© Topic 4: Memory in LangChain (Modern Runnable-Compatible Approach)\n",
    "\n",
    "Memory lets a conversation-based app **remember context across turns**.  \n",
    "Without it, the model would treat every prompt as independent.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Why Memory Matters\n",
    "| Without Memory | With Memory |\n",
    "|----------------|-------------|\n",
    "| ‚ÄúWho are you?‚Äù ‚Üí ‚ÄúI‚Äôm Mistral.‚Äù<br>‚ÄúWhat‚Äôs my name?‚Äù ‚Üí ‚ÄúI don‚Äôt know.‚Äù | ‚ÄúWho are you?‚Äù ‚Üí ‚ÄúI‚Äôm Mistral.‚Äù<br>‚ÄúWhat‚Äôs my name?‚Äù ‚Üí ‚ÄúYou‚Äôre Alex.‚Äù |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Types of Memory (Conceptually)\n",
    "| Memory Type | What It Stores | Typical Use |\n",
    "|--------------|---------------|--------------|\n",
    "| **ConversationBufferMemory** | Full conversation transcript | Short chats |\n",
    "| **ConversationSummaryMemory** | Summarized conversation | Long-running sessions |\n",
    "| **ConversationTokenBufferMemory** | Recent tokens only | Context window management |\n",
    "| **EntityMemory** | Facts about entities (people, places, etc.) | Personalized chatbots |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How Memory Works Now (2025+)\n",
    "Earlier versions used `ConversationChain(memory=...)`.  \n",
    "Now we can plug memories directly into **Runnables** or **Agent Executors** by manually managing the stored messages.\n",
    "\n",
    "In other words:  \n",
    "> ‚ÄúMemory is just structured context we append to prompts before calling the LLM.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful IT assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def conversation_step(user_input):\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    formatted = prompt.format_messages(history=history, input=user_input)\n",
    "    reply = llm.invoke(formatted)\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": reply})\n",
    "    return reply\n",
    "\n",
    "print(conversation_step(\"Hi, who are you?\"))\n",
    "print(conversation_step(\"Can you remind me what I just asked?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263a99f",
   "metadata": {},
   "source": [
    "Buffer Memory with Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e4a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am an AI-powered IT Support Assistant designed to help you with your technical issues and provide guidance on various technology-related topics. How can I assist you today?\n",
      " Yes, you asked if I could tell you what you just asked. Your original question was \"Hi, who are you?\" which I responded to by telling you that I am an AI-powered IT Support Assistant designed to help with technology issues and provide guidance on various topics. In your second question, you were asking the same question again, but rephrased as \"Can you tell me what I just asked?\" My response is intended to confirm that you are indeed asking for a repetition of your original question.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "history = []\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful IT support assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def memory_step(user_input):\n",
    "    \"\"\"Simulate buffer memory manually.\"\"\"\n",
    "    history.append((\"human\", user_input))\n",
    "    messages = prompt.format_messages(history=history, input=user_input)\n",
    "    reply = llm.invoke(messages)\n",
    "    history.append((\"assistant\", reply))\n",
    "    return reply\n",
    "\n",
    "print(memory_step(\"Hi, who are you?\"))\n",
    "print(memory_step(\"Can you tell me what I just asked?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50e25a",
   "metadata": {},
   "source": [
    "Summarized (Condensed) Memory using Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34105e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cloud Computing is a model for delivering on-demand computer system resources over the internet, with the goal of providing scalable and self-service IT resources to users. These resources include servers, storage, databases, networking, software, analytics, Intelligence and everything needed to run an application or a business. The \"cloud\" refers to the software and services available online, which are managed remotely on shared servers rather than a local server or a personal computer. This allows for greater flexibility, cost savings, scalability, collaboration, and overall efficiency in computing tasks.\n",
      " Cloud Storage and Local Storage are two distinct methods of data storage. Here are the key differences between them:\n",
      "\n",
      "1. Location: Local Storage is a physical device, such as a hard drive on your computer or an external USB drive, where you store files and data directly on your device. On the other hand, Cloud Storage involves storing data on servers managed by third-party providers over the internet.\n",
      "\n",
      "2. Accessibility: With Local Storage, you can only access your data when you are physically present with the device containing the storage. In contrast, Cloud Storage allows you to access your files and data from any device connected to the internet, as long as you have the appropriate credentials.\n",
      "\n",
      "3. Scalability: When it comes to storage capacity, Local Storage has a limited capacity defined by the size of the physical drive. To expand local storage, you need to purchase additional drives or upgrade existing ones. Cloud Storage, however, offers scalable storage solutions, where you can easily increase your storage capacity as needed without the need for hardware upgrades.\n",
      "\n",
      "4. Security: Both options have their own security considerations. Local Storage may be more susceptible to physical theft or damage. While cloud storage providers implement robust security measures to protect data, there is always a risk of data breaches and unauthorized access. It's essential to choose a reputable provider and take additional precautions such as using strong passwords, two-factor authentication, and encrypting sensitive files.\n",
      "\n",
      "5. Cost: Local Storage may require an upfront cost for the physical device and its maintenance over time. Cloud Storage, on the other hand, typically involves a subscription fee that can be more affordable in the long run, especially when considering scalability and ongoing maintenance costs.\n",
      "\n",
      "6. Backup and Recovery: Cloud Storage providers often include backup and disaster recovery solutions as part of their offerings, making it easier to recover lost or corrupted data compared to local storage options, which may require additional investment in backup software and external drives.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summary = None\n",
    "\n",
    "def summarize_history(history):\n",
    "    \"\"\"Summarize long conversation using LLM.\"\"\"\n",
    "    text = \"\\n\".join([f\"{r}: {m}\" for r, m in history])\n",
    "    return llm.invoke(f\"Summarize the following conversation in 4-5 lines:\\n{text}\")\n",
    "\n",
    "def chat_with_summary(user_input):\n",
    "    global summary\n",
    "\n",
    "    # If there‚Äôs a summary, include it before the chat\n",
    "    system_prompt = \"You are a concise IT assistant.\"\n",
    "    history_msgs = history.copy()\n",
    "    if summary:\n",
    "        history_msgs.insert(0, (\"system\", f\"Summary so far: {summary}\"))\n",
    "\n",
    "    history_msgs.append((\"human\", user_input))\n",
    "    messages = prompt.format_messages(history=history_msgs, input=user_input)\n",
    "    reply = llm.invoke(messages)\n",
    "    history.append((\"assistant\", reply))\n",
    "\n",
    "    # Summarize if conversation gets too long\n",
    "    if len(history) > 8:\n",
    "        summary = summarize_history(history)\n",
    "        history.clear()\n",
    "        history.append((\"system\", f\"Condensed summary: {summary}\"))\n",
    "\n",
    "    return reply\n",
    "\n",
    "print(chat_with_summary(\"What is cloud computing?\"))\n",
    "print(chat_with_summary(\"Explain how it's storage differs from local storage.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = None\n",
    "\n",
    "def hybrid_chat(user_input):\n",
    "    global summary\n",
    "    # keep only last 3 exchanges\n",
    "    short_history = history[-6:]\n",
    "\n",
    "    combined = []\n",
    "    if summary:\n",
    "        combined.append((\"system\", f\"Earlier summary: {summary}\"))\n",
    "    combined += short_history\n",
    "    combined.append((\"human\", user_input))\n",
    "\n",
    "    messages = prompt.format_messages(history=combined, input=user_input)\n",
    "    reply = llm.invoke(messages)\n",
    "    history.append((\"assistant\", reply))\n",
    "\n",
    "    # update summary every 6 messages\n",
    "    if len(history) % 6 == 0:\n",
    "        summary = summarize_history(history)\n",
    "\n",
    "    return reply\n",
    "\n",
    "print(hybrid_chat(\"Tell me about virtualization.\"))\n",
    "print(hybrid_chat(\"How does it compare to containerization?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(history, indent=2))\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa07c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "history = []\n",
    "\n",
    "# summarizer Runnable\n",
    "summarizer = RunnableLambda(lambda x: llm.invoke(f\"Summarize this chat:\\n{x}\"))\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a concise IT assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "def chat_step(user_input):\n",
    "    # append new turn\n",
    "    history.append((\"human\", user_input))\n",
    "    formatted = prompt.format_messages(history=history, input=user_input)\n",
    "    reply = llm.invoke(formatted)\n",
    "    history.append((\"assistant\", reply))\n",
    "\n",
    "    # if history too long ‚Üí condense\n",
    "    if len(history) > 10:\n",
    "        summary_text = summarizer.invoke(str(history))\n",
    "        history.clear()\n",
    "        history.append((\"system\", f\"Summary so far: {summary_text}\"))\n",
    "\n",
    "    return reply\n",
    "\n",
    "print(chat_step(\"Explain virtualization.\"))\n",
    "print(chat_step(\"And how is it different from containerization?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª IT FAQ Assistant (Runnable version)\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import json, os\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load / Save History (same)\n",
    "# ------------------------------\n",
    "HISTORY_FILE = \"faq_chat.json\"\n",
    "\n",
    "def load_chat():\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        with open(HISTORY_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                return json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "def save_chat(messages):\n",
    "    with open(HISTORY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(messages, f, indent=2)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Define LLM + Prompt (Runnable)\n",
    "# -----------------------------------\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful IT FAQ assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Chat loop using Runnable logic\n",
    "# -----------------------------------\n",
    "history = load_chat()\n",
    "\n",
    "def chat_step(user_input):\n",
    "    # Prepare message history for LangChain\n",
    "    formatted_messages = [(m[\"role\"], m[\"content\"]) for m in history]\n",
    "    formatted_messages.append((\"human\", user_input))\n",
    "\n",
    "    # Format + run through LLM\n",
    "    messages = prompt.format_messages(history=formatted_messages, input=user_input)\n",
    "    reply = llm.invoke(messages)\n",
    "\n",
    "    # Update and persist\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    save_chat(history)\n",
    "    return reply\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Main Interaction\n",
    "# -----------------------------------\n",
    "print(\"üíª IT FAQ Assistant (Runnable version)\\nType 'exit' to quit.\\n\")\n",
    "while True:\n",
    "    user = input(\"You: \").strip()\n",
    "    if user.lower() == \"exit\":\n",
    "        save_chat(history)\n",
    "        break\n",
    "    print(\"Assistant:\", chat_step(user))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
