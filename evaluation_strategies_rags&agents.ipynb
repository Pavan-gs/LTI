{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ebee72e3-83f5-4568-8c72-8020664c0c32",
   "metadata": {},
   "source": [
    "How do we evaluate quality of responses?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f86452a-1915-46ca-886d-5a09ead000a5",
   "metadata": {},
   "source": [
    "| Evaluation Type   | Metrics/Method           | Used For                              |\n",
    "| ----------------- | ------------------------ | ------------------------------------- |\n",
    "| Retrieval Quality | Precision@k, Recall@k    | Did RAG fetch correct chunks?         |\n",
    "| Response Quality  | ROUGE/L, BLEU, BERTScore | Did LLM produce relevant answer?      |\n",
    "| Truthfulness      | Fact-check against docs  | No hallucination?                     |\n",
    "| Task Success      | Human evaluation sheets  | Did the assistant solve the use-case? |\n",
    "| Robustness        | Adversarial prompts      | Does it break easily?                 |\n",
    "| Latency           | Avg response time        | UX performance                        |\n",
    "| Usage Logs        | Query → retrieved docs   | Debugging relevance                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d4941-3b1a-45fb-bf45-400fd42a7c40",
   "metadata": {},
   "source": [
    "Example retrieval test:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "144fb367-0abd-4568-b83c-cc04f3de7b62",
   "metadata": {},
   "source": [
    "Query: \"How to reset AC?\"\n",
    "RAG returned doc with reset steps? → pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626970a4-36f0-467e-bbbb-3a9fc96b18c4",
   "metadata": {},
   "source": [
    "Example response test using BLEU/ROUGE:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b89f4697-ec35-4dc7-8216-5862dd4dc1c3",
   "metadata": {},
   "source": [
    "Expected steps vs model output → similarity score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f1ed5-51a0-4395-b7de-db1f94433325",
   "metadata": {},
   "source": [
    "Optimization in RAG-based AI systems"
   ]
  },
  {
   "cell_type": "raw",
   "id": "605a9157-392e-4cd2-b50d-2cae72c5386c",
   "metadata": {},
   "source": [
    "Like Optuna/GridSearch for ML, how do we optimize LLM + RAG + Agents?\n",
    "We tune retrieval + prompting + memory + workflow, not weights.\n",
    "\n",
    "⚙ Optimization knobs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bd0ca35-f8fa-43b2-afcc-1d79ee7e3bf1",
   "metadata": {},
   "source": [
    "| Component       | Tunable aspects             | Methods                                 |\n",
    "| --------------- | --------------------------- | --------------------------------------- |\n",
    "| RAG Retrieval   | top_k, chunk size, overlap  | Search grid `k=2–10` evaluate precision |\n",
    "| Embedding Model | Try multiple                | Compare cosine similarity scores        |\n",
    "| Vector Storage  | FAISS vs Chroma             | Measure latency                         |\n",
    "| Prompts         | System instructions, format | Prompt AB testing                       |\n",
    "| Tools           | Rule thresholds, caching    | Optimize logic                          |\n",
    "| Agents          | Decision routing            | Workflow success scoring                |\n",
    "| Caching         | TTL, response store         | Reduce latency                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b6cb4-ed6b-4d0d-b266-99a6b9c7f039",
   "metadata": {},
   "source": [
    "Techniques similar to ML tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3baedaa8-4850-4ebf-afa8-ccb95faed67f",
   "metadata": {},
   "source": [
    "| Analog to ML          | For RAG Optimization                           |\n",
    "| --------------------- | ---------------------------------------------- |\n",
    "| Hyperparameter search | Search best `chunk_size`, `k`, overlap         |\n",
    "| Cross validation      | Test retrieval accuracy on labeled dataset     |\n",
    "| Early stopping        | Stop expanding chunk size if performance drops |\n",
    "| Loss curves           | Monitor precision metrics over configs         |\n",
    "| Grid/Random search    | Evaluate multiple prompt variants              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbb0fd-57d2-4401-8c73-85b9fbd4cd40",
   "metadata": {},
   "source": [
    "Example optimization experiment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6981b634-918c-425d-ab06-19f15800d3be",
   "metadata": {},
   "source": [
    "chunk_size = [200, 350, 500]\n",
    "overlap = [30, 60, 90]\n",
    "k = [3, 5, 7]\n",
    "\n",
    "Run retrieval test dataset of 50 queries → compute P@3.\n",
    "Select best configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192945e-80a2-417b-8c9b-bdea396b24ff",
   "metadata": {},
   "source": [
    "Example --> LEGAL CONTRACT ASSISTANT — Evaluation + Optimization Framework"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ce5bb2a-403d-4fd0-a59f-f97d04b116fe",
   "metadata": {},
   "source": [
    "This template tells how to evaluate the legal agent’s response quality, how to check reliability of RAG retrieval, and how to optimize the performance of the system similar to ML hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf22a79-8a32-458a-b793-2ffb37be060d",
   "metadata": {},
   "source": [
    "EVALUATION DIMENSIONS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b889533-74a6-4770-94ed-2eda9136240f",
   "metadata": {},
   "source": [
    "Legal assistants must be evaluated across four core criteria:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "372394d2-e84a-4e4b-bc23-c5447f24f9e2",
   "metadata": {},
   "source": [
    "| Category                    | What to Measure                                       | Why it Matters                     |\n",
    "| --------------------------- | ------------------------------------------------------|--------------------------------\n",
    "| **RAG Retrieval Quality**   | Is the correct clause returned? Is retrieval relevant?|Prevent false legal interpretation |\n",
    "| **LLM Quality**             | Clarity, correctness, non-hallucination               | Ensures safe recommendations       |\n",
    "| **Tool Reasoning Accuracy** | Risk classification, clause extraction, missing clause detection | Ensure deterministic reliability   |\n",
    "| **System Performance**      | Latency, throughput, memory usage                     | Ensures production usability      \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3144aa4-9055-4f4e-a33e-448d674bc0a0",
   "metadata": {},
   "source": [
    "Dataset for Evaluation\n",
    "Students should prepare a small curated evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d43580-f4b3-45bf-8343-e5e003ee2889",
   "metadata": {},
   "source": [
    "Example evaluation set:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6688330e-f571-437c-acbe-03129c850bed",
   "metadata": {},
   "source": [
    "/evaluation/legal_eval/\n",
    "    clauses_test_questions.json\n",
    "    expected_responses.json\n",
    "    high_risk_clauses.txt\n",
    "    safe_precedent_clauses.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c36d5-d66d-4c09-ba10-f0a18501dfe9",
   "metadata": {},
   "source": [
    "Example clauses_test_questions.json:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a9f49c2-fb6c-410d-b8e1-09b30d5164b4",
   "metadata": {},
   "source": [
    "[\n",
    "  {\n",
    "    \"query\": \"What is the liability clause in this contract?\",\n",
    "    \"expected_metadata\": [\"liability\", \"risk\"],\n",
    "    \"expected_response_contains\": [\"liability cap\", \"direct damages\"]\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"Is this termination clause risky?\",\n",
    "    \"expected_keywords\": [\"notice period\", \"unilateral\"]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74a619-a4c3-42eb-a432-391d8d6ca4bf",
   "metadata": {},
   "source": [
    "#### RAG Evaluation Metrics\n",
    "##### Retrieval Precision@K"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e77e1b-d6bb-4648-97db-609a506e7345",
   "metadata": {},
   "source": [
    "precision@k = (# relevant chunks retrieved) / k"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3fb161f-b39d-478a-8c59-9c07af2e55d7",
   "metadata": {},
   "source": [
    "✔ Used to test if RAG retrieval pulls correct clause sections.\n",
    "\n",
    "Example threshold:\n",
    "Good: Precision@3 > 0.8\n",
    "Acceptable: > 0.6\n",
    "Needs improvement: < 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17b34a-c3f9-4a9d-b4ce-72f529d19c7a",
   "metadata": {},
   "source": [
    "##### Recall@K"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88553c7d-2ae9-4eac-a2b7-c8d97269241e",
   "metadata": {},
   "source": [
    "If 5 relevant clauses exist & only 3 are retrieved → recall=0.6.\n",
    "Used to test coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0179b5-369b-47b7-b505-e19810f83013",
   "metadata": {},
   "source": [
    "##### MRR (Mean Reciprocal Rank)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4b30172-69f3-4370-a60a-967b74459566",
   "metadata": {},
   "source": [
    "1 / rank of first correct retrieved chunk"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34b4da14-cd1d-4b30-a796-5ef6f7939eb3",
   "metadata": {},
   "source": [
    "Measures how high the right clause appears in results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043d66b-f83b-4bc1-ba1f-700c6264c653",
   "metadata": {},
   "source": [
    "Semantic Similarity (Cosine score on embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46bafb91-b737-4658-b85a-ec55e3f7efd6",
   "metadata": {},
   "source": [
    "cosine(chunk_embedding, expected_clause_embedding) > 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec34c98-3351-41f9-ab8c-b4c8e3f43e4d",
   "metadata": {},
   "source": [
    "LLM Response Quality Evaluation Metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6313cbdb-2a9f-4655-87de-3a773525a0f1",
   "metadata": {},
   "source": [
    "Same BLEU/ROUGE used in NLP.\n",
    "\n",
    "| Metric                   | Purpose                                  |\n",
    "| ------------------------ | ---------------------------------------- |\n",
    "| **ROUGE-L**              | Structural overlap with reference answer |\n",
    "| **BLEU**                 | Wording similarity                       |\n",
    "| **BERTScore** (optional) | Meaning similarity                       |"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f1ad2ac-d883-4e52-8341-a5de3925c67e",
   "metadata": {},
   "source": [
    "Evaluate with:\n",
    "\n",
    "answers_model.txt\n",
    "answers_human.txt\n",
    "\n",
    "Use scripts/notebooks in /evaluation/notebooks/."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0ef7152-925a-41e9-94ae-fa95a753e1b2",
   "metadata": {},
   "source": [
    "Subjective Human Scoring Rubric (Recommended)\n",
    "\n",
    "| Score | Meaning                                     |\n",
    "| ----- | ------------------------------------------- |\n",
    "| 5     | Correct, complete, actionable, legally safe |\n",
    "| 4     | Mostly correct, minor safety missing        |\n",
    "| 3     | Helpful but insufficient detail             |\n",
    "| 2     | Misleading or incomplete                    |\n",
    "| 1     | Wrong or risky → FAIL                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014823e-a192-49a0-a652-0e5a4cf37b4d",
   "metadata": {},
   "source": [
    "Legal Hallucination Check"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78152887-a5f8-41a9-b266-e89e5688260a",
   "metadata": {},
   "source": [
    "LLM may invent:\n",
    "\n",
    "✘ fake laws\n",
    "✘ wrong deadlines\n",
    "✘ incorrect liability interpretations\n",
    "\n",
    "So evaluation must include:\n",
    "\n",
    "| Checkpoint                       | Example                                    |\n",
    "| -------------------------------- | ------------------------------------------ |\n",
    "| Verify clause exists in document | “Locate the termination clause”            |\n",
    "| Force RAG grounding              | “Show reference text used for this answer” |\n",
    "| Prove extraction correctness     | “Quote exact text snippet”                 |\n",
    "| Compliance validation            | “Cite section & sub-section numbers”       |"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d22c38da-70a5-4e37-b1a7-ff17ff782f7f",
   "metadata": {},
   "source": [
    "Add a “Source Required” mode:\n",
    "\n",
    "return answer + retrieved_clause_text + document_reference\n",
    "If answer isn’t backed by retrieved text → mark as hallucination."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb309e3b-07f8-4bf7-a45b-2f99de85b290",
   "metadata": {},
   "source": [
    "#### Optimization Strategy — Like Hyperparameter Tuning but for RAG & Agents"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9747f61d-b39c-48c2-981f-081e64875a3e",
   "metadata": {},
   "source": [
    "Below are the optimization knobs analogous to learning rate, batch size, regularization in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e3277-6722-49d0-8942-9fe8800ffcdd",
   "metadata": {},
   "source": [
    "#### RAG Optimization Variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7569d458-d3a9-4a91-9d80-18f9bcd13a65",
   "metadata": {},
   "source": [
    "| Parameter          | What to Tune                                    | Range          |\n",
    "| ------------------ | ----------------------------------------------- | -------------- |\n",
    "| Chunk Size         | Larger = more context, Smaller = more precision | 200–500 tokens |\n",
    "| Chunk Overlap      | Prevent context loss                            | 40–100 tokens  |\n",
    "| Top-K              | Too low misses info, too high adds noise        | 3–8            |\n",
    "| Embedding Model    | Better embeddings → better retrieval            | Compare models |\n",
    "| Metadata Filtering | Applying tags reduces noise                     | Enable/Disable |\n",
    "| Reranking          | Use cross-encoders to improve ranking           | Optional layer |"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4cf02be-13bf-439e-9c3e-efd83efed2fa",
   "metadata": {},
   "source": [
    "Procedure (like grid search):\n",
    "\n",
    "for chunk_size in [200,300,400]:\n",
    "  for top_k in [3,5,7]:\n",
    "    evaluate_precision_recall()\n",
    "\n",
    "Pick configuration with best Precision@K + MRR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8cf1c-7486-4460-9250-65f482046dae",
   "metadata": {},
   "source": [
    "Agent Optimization Variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30626a84-4a74-4475-a1d5-0dfa5d84d46e",
   "metadata": {},
   "source": [
    "| Parameter        | Options                   |\n",
    "| ---------------- | ------------------------- |\n",
    "| Routing logic    | rule-based / LLM-based    |\n",
    "| Number of agents | 2 simple → 6 advanced     |\n",
    "| Tool call style  | multi-step or parallel    |\n",
    "| Memory choice    | entity + summary + vector |"
   ]
  },
  {
   "cell_type": "raw",
   "id": "231ffdb6-b9e0-48f8-9d5f-d3454f424fc3",
   "metadata": {},
   "source": [
    "Optimize for:\n",
    "\n",
    "shortest reasoning path\n",
    "minimal hallucination\n",
    "fastest resolution steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c3bf9-f135-41d2-9e98-3250a7c7db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM Optimization Variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
